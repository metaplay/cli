/*
 * Copyright Metaplay. Licensed under the Apache-2.0 license.
 */

package cmd

import (
	"archive/zip"
	"bytes"
	"compress/gzip"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"regexp"
	"strings"
	"time"

	"github.com/metaplay/cli/pkg/envapi"
	"github.com/metaplay/cli/pkg/kubeutil"
	"github.com/metaplay/cli/pkg/styles"
	"github.com/rs/zerolog/log"
	"github.com/spf13/cobra"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/kubernetes/scheme"
)

// databaseExportOpts holds the options for the 'database export' command
type databaseExportOpts struct {
	UsePositionalArgs

	// Environment and output file
	argEnvironment string
	argOutputFile  string
}

func init() {
	o := databaseExportOpts{}

	args := o.Arguments()
	args.AddStringArgument(&o.argEnvironment, "ENVIRONMENT", "Target environment name or id, eg, 'lovely-wombats-build-nimbly'.")
	args.AddStringArgumentOpt(&o.argOutputFile, "OUTPUT_FILE", "Output file path for the database snapshot. Defaults to 'database-snapshot-${env}-${timestamp}.mdb' if not specified.")

	cmd := &cobra.Command{
		Use:     "export [ENVIRONMENT] [OUTPUT_FILE] [flags]",
		Aliases: []string{"exp"},
		Short:   "[preview] Export database snapshot from an environment",
		Long: renderLong(&o, `
			PREVIEW: This is a preview feature and interface may change in the future.

			Export a full database snapshot from the specified environment using mariadb-dump.

			DISCLAIMER: This operation is not a a real backup and is intended for ad hoc database
			exports only. This operation should only be used for relatively small databases as it
			copies the entire database to the machine running the CLI.

			The generated snapshot is a zip file containing metadata about the snapshot and a compressed
			SQL dump for each shard, generated by mariadb-dump. The format of the snapshot is subject
			to change in the future.

			This command starts a temporary debug pod and runs mariadb-dump inside it, connects
			to the read-only replica of each shard of the database and creates a complete snapshot.

			{Arguments}

			Related commands:
			- 'metaplay database import' imports a database snapshot into an environment.
			- 'metaplay debug database' connects to a database shard interactively.
		`),
		Example: renderExample(`
			# Export database from 'nimbly' environment (uses default filename)
			metaplay database export nimbly

			# Export database to a specific file
			metaplay database export nimbly my_database_snapshot.mdb
		`),
		Run: runCommand(&o),
	}
	databaseCmd.AddCommand(cmd)
}

func (o *databaseExportOpts) Prepare(cmd *cobra.Command, args []string) error {
	// Generate default output file if not specified
	if o.argOutputFile == "" {
		timestamp := time.Now().Format("20060102-150405")
		o.argOutputFile = fmt.Sprintf("database-snapshot-%s-%s.mdb", o.argEnvironment, timestamp)
	}
	return nil
}

func (o *databaseExportOpts) Run(cmd *cobra.Command) error {
	// Resolve the project & auth provider
	project, err := tryResolveProject()
	if err != nil {
		return err
	}

	// Resolve environment config
	envConfig, tokenSet, err := resolveEnvironment(cmd.Context(), project, o.argEnvironment)
	if err != nil {
		return err
	}

	// Resolve target environment & game server
	targetEnv := envapi.NewTargetEnvironment(tokenSet, envConfig.StackDomain, envConfig.HumanID)
	kubeCli, err := targetEnv.GetPrimaryKubeClient()
	if err != nil {
		return err
	}

	// Fetch the database shard configuration from Kubernetes secret
	log.Debug().Str("namespace", kubeCli.Namespace).Msg("Fetching database shard configuration")
	shards, err := kubeutil.FetchDatabaseShardsFromSecret(cmd.Context(), kubeCli, kubeCli.Namespace)
	if err != nil {
		return err
	}

	// Validate that we have at least one shard
	if len(shards) == 0 {
		return fmt.Errorf("no database shards found in environment")
	}

	// Fill in shard indices
	for shardNdx := range shards {
		shards[shardNdx].ShardIndex = shardNdx
	}

	// Show info
	log.Info().Msg("")
	log.Info().Msg("Database export info:")
	log.Info().Msgf("  Environment: %s", styles.RenderTechnical(o.argEnvironment))
	log.Info().Msgf("  Shards:      %s", styles.RenderTechnical(fmt.Sprintf("%d", len(shards))))
	log.Info().Msgf("  Output file: %s", styles.RenderTechnical(o.argOutputFile))
	log.Info().Msg("")

	// Create a debug container to run mariadb-dump
	log.Debug().Msg("Creating debug pod for database export")
	podName, cleanup, err := kubeutil.CreateDebugPod(
		cmd.Context(),
		kubeCli,
		debugDatabaseImage,
		false,
		false,
		[]string{"sleep", "600"}, // 10min is enough for modestly-sized databases
	)
	if err != nil {
		return err
	}
	log.Debug().Str("pod_name", podName).Msg("Debug pod created successfully")
	// Make sure the debug container is cleaned up even if we return early
	defer cleanup()

	// Export the database
	log.Debug().Str("output_file", o.argOutputFile).Msg("Start database export")
	err = o.exportDatabaseContents(cmd.Context(), kubeCli, podName, "debug", shards)
	if err != nil {
		// Hard failure - remove incomplete file
		os.Remove(o.argOutputFile)
		log.Error().Err(err).Msg("Database export failed - removing incomplete zip file")
		return fmt.Errorf("CRITICAL: database export failed, zip file removed: %v", err)
	}

	log.Info().Msg("")
	log.Info().Msgf("âœ… Database export completed successfully")

	return nil
}

// DatabaseSnapshotMetadata contains information about the database export
type DatabaseSnapshotMetadata struct {
	Version       int       `json:"version"`
	Environment   string    `json:"environment"`
	DatabaseName  string    `json:"database_name"`
	NumShards     int       `json:"num_shards"`
	ExportedAt    time.Time `json:"exported_at"`
	ExportOptions string    `json:"export_options"`
}

// Main function to export database contents - creates zip file, writes metadata, and exports all shards
func (o *databaseExportOpts) exportDatabaseContents(ctx context.Context, kubeCli *envapi.KubeClient, podName, debugContainerName string, shards []kubeutil.DatabaseShardConfig) error {
	log.Info().Msgf("Exporting database...")
	exportOptions := "--routines --triggers --no-tablespaces"

	// Create output zip file
	log.Debug().Str("zip_file", o.argOutputFile).Msg("Creating output zip file")
	zipFile, err := os.Create(o.argOutputFile)
	if err != nil {
		return fmt.Errorf("failed to create zip file: %v", err)
	}
	defer zipFile.Close()

	// Create zip writer
	zipWriter := zip.NewWriter(zipFile)
	defer zipWriter.Close()

	// Write metadata to zip
	log.Debug().Msg("Writing metadata to zip file")
	err = o.writeMetadataToZip(zipWriter, shards, exportOptions)
	if err != nil {
		return fmt.Errorf("failed to write metadata: %v", err)
	}

	// Extract schema from shard #0 first
	log.Debug().Msg("Extracting database schema from shard #0")
	schemaContent, err := o.extractDatabaseSchema(ctx, kubeCli, podName, debugContainerName, exportOptions, shards[0])
	if err != nil {
		return fmt.Errorf("failed to extract schema: %v", err)
	}

	// Apply schema fixups
	schemaContent = o.applySchemaFixups(schemaContent)

	// Write schema to zip file
	err = o.writeSchemaToZip(zipWriter, schemaContent)
	if err != nil {
		return fmt.Errorf("failed to write schema to zip: %v", err)
	}

	// Export data from each shard
	var shardFileNames []string
	for _, shard := range shards {
		log.Debug().Int("shard_index", shard.ShardIndex).Str("database_name", shard.DatabaseName).Msg("Starting shard data export")
		shardFileName, err := o.exportDatabaseShardData(ctx, zipWriter, kubeCli, podName, debugContainerName, shard)
		if err != nil {
			return fmt.Errorf("failed to export shard %d data: %v", shard.ShardIndex, err)
		}
		log.Debug().Int("shard_index", shard.ShardIndex).Str("shard_file", shardFileName).Msg("Shard data export completed")
		shardFileNames = append(shardFileNames, shardFileName)
	}

	return nil
}

// Helper function to write metadata to zip file
func (o *databaseExportOpts) writeMetadataToZip(zipWriter *zip.Writer, shards []kubeutil.DatabaseShardConfig, exportOptions string) error {

	// Use first shard for database name (all shards should have same database name)
	databaseName := ""
	if len(shards) > 0 {
		databaseName = shards[0].DatabaseName
	}

	// Create metadata
	log.Debug().Str("database_name", databaseName).Int("num_shards", len(shards)).Msg("Creating export metadata")
	metadata := DatabaseSnapshotMetadata{
		Version:       1,
		Environment:   o.argEnvironment,
		DatabaseName:  databaseName,
		NumShards:     len(shards),
		ExportedAt:    time.Now().UTC(),
		ExportOptions: exportOptions,
	}

	// Create metadata JSON in memory
	metadataBytes, err := json.MarshalIndent(metadata, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal metadata: %v", err)
	}

	// Write metadata file to zip
	metadataFileName := "metadata.json"
	metadataHeader := &zip.FileHeader{
		Name:     metadataFileName,
		Method:   zip.Store, // No compression for small metadata file
		Modified: time.Now(),
	}
	metadataHeader.SetMode(0644)

	// Write metadata file to zip
	metadataWriter, err := zipWriter.CreateHeader(metadataHeader)
	if err != nil {
		return fmt.Errorf("failed to create metadata writer: %v", err)
	}
	if _, err := metadataWriter.Write(metadataBytes); err != nil {
		return fmt.Errorf("failed to write metadata content: %v", err)
	}

	return nil
}

// Helper function to extract database schema from shard #0 into memory
func (o *databaseExportOpts) extractDatabaseSchema(ctx context.Context, kubeCli *envapi.KubeClient, podName, debugContainerName, exportOptions string, shard kubeutil.DatabaseShardConfig) (string, error) {
	// Build mariadb-dump command for schema only (DDL) - no gzip compression for in-memory processing
	schemaCmd := fmt.Sprintf("mariadb-dump -h %s -u %s -p%s --no-create-db --no-data %s %s",
		shard.ReadOnlyHost, // Use read-only replica
		shard.UserId,
		shard.Password,
		exportOptions,
		shard.DatabaseName)
	log.Debug().Str("host", shard.ReadOnlyHost).Str("database", shard.DatabaseName).Msg("Executing schema dump command")

	// Capture output in memory buffer
	var schemaBuffer bytes.Buffer

	// Execute schema dump command
	req := kubeCli.Clientset.CoreV1().
		RESTClient().
		Post().
		Resource("pods").
		Name(podName).
		Namespace(kubeCli.Namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: debugContainerName,
			Command:   []string{"/bin/sh", "-c", schemaCmd},
			Stdin:     false,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, scheme.ParameterCodec)

	ioStreams := IOStreams{
		In:     nil,
		Out:    &schemaBuffer, // Capture to memory buffer
		ErrOut: os.Stderr,
	}

	err := execRemoteKubernetesCommand(ctx, kubeCli.RestConfig, req.URL(), ioStreams, false, false)
	if err != nil {
		log.Error().Err(err).Msg("Schema extraction failed - aborting export")
		return "", fmt.Errorf("CRITICAL: schema extraction failed: %v", err)
	}

	schemaContent := schemaBuffer.String()
	log.Debug().Int("schema_size", len(schemaContent)).Msg("Schema extracted to memory successfully")

	return schemaContent, nil
}

// Helper function to apply schema modifications and fixups
func (o *databaseExportOpts) applySchemaFixups(schemaContent string) string {
	log.Debug().Msgf("Original schema from mariadb-dump:\n%s", schemaContent)

	// Ensure all tables use utf8mb4_bin collation
	schemaContent = o.enforceUtf8mb4BinCollation(schemaContent)

	log.Debug().Msg("Applied schema fixups")
	return schemaContent
}

// Helper function to enforce utf8mb4_bin collation on all tables
func (o *databaseExportOpts) enforceUtf8mb4BinCollation(schemaContent string) string {
	// Simple approach: replace all collation references with utf8mb4_bin

	// 1. Replace table-level collation in CREATE TABLE statements
	tableCollationPattern := regexp.MustCompile(`(?i)DEFAULT\s+CHARSET=\w+\s+COLLATE=\w+`)
	result := tableCollationPattern.ReplaceAllString(schemaContent, "DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin")

	// 2. Replace column-level collation specifications
	columnCollationPattern := regexp.MustCompile(`(?i)CHARACTER\s+SET\s+\w+\s+COLLATE\s+\w+`)
	result = columnCollationPattern.ReplaceAllString(result, "CHARACTER SET utf8mb4 COLLATE utf8mb4_bin")

	// 3. Replace standalone COLLATE clauses
	standaloneCollatePattern := regexp.MustCompile(`(?i)COLLATE\s+\w+`)
	result = standaloneCollatePattern.ReplaceAllString(result, "COLLATE utf8mb4_bin")

	// 4. Replace standalone CHARSET clauses
	standaloneCharsetPattern := regexp.MustCompile(`(?i)CHARSET=\w+`)
	result = standaloneCharsetPattern.ReplaceAllString(result, "CHARSET=utf8mb4")

	log.Debug().Msg("Enforced utf8mb4_bin collation on all tables and columns")
	return result
}

// Helper function to write schema content to zip file
func (o *databaseExportOpts) writeSchemaToZip(zipWriter *zip.Writer, schemaContent string) error {
	// Prepare zip header for schema file
	schemaHeader := &zip.FileHeader{
		Name:     "schema.sql",
		Method:   zip.Deflate, // Use compression since we're not pre-compressing
		Modified: time.Now(),
	}
	schemaHeader.SetMode(0644)

	// Create zip writer for schema file
	schemaWriter, err := zipWriter.CreateHeader(schemaHeader)
	if err != nil {
		return fmt.Errorf("failed to create schema writer: %v", err)
	}

	// Write schema content to zip
	_, err = schemaWriter.Write([]byte(schemaContent))
	if err != nil {
		return fmt.Errorf("failed to write schema content: %v", err)
	}

	log.Debug().Msg("Schema written to schema.sql in zip archive")
	return nil
}

// Helper function to export data only from a single database shard
func (o *databaseExportOpts) exportDatabaseShardData(ctx context.Context, zipWriter *zip.Writer, kubeCli *envapi.KubeClient, podName, debugContainerName string, shard kubeutil.DatabaseShardConfig) (string, error) {
	// Build mariadb-dump command for data only (DML) with proper gzip termination
	// Note: Using --hex-blob as raw binary causes failures on import.
	dataCmd := fmt.Sprintf("mariadb-dump -h %s -u %s -p%s --no-create-info --no-tablespaces --single-transaction --hex-blob --skip-triggers %s",
		shard.ReadOnlyHost, // Use read-only replica
		shard.UserId,
		shard.Password,
		shard.DatabaseName)
	log.Debug().Str("host", shard.ReadOnlyHost).Str("database", shard.DatabaseName).Msg("Executing data dump command")

	// Prepare zip header for streaming
	shardFileName := fmt.Sprintf("shard_%d.sql", shard.ShardIndex)
	shardHeader := &zip.FileHeader{
		Name:     shardFileName,
		Method:   zip.Deflate, // Use compression since data is not pre-compressed
		Modified: time.Now(),
	}
	shardHeader.SetMode(0644)

	// Create zip writer for this file - stream directly without buffering
	shardFileWriter, err := zipWriter.CreateHeader(shardHeader)
	if err != nil {
		return "", fmt.Errorf("failed to create zip writer: %v", err)
	}

	// Execute mariadb-dump command and stream output directly to zip
	req := kubeCli.Clientset.CoreV1().
		RESTClient().
		Post().
		Resource("pods").
		Name(podName).
		Namespace(kubeCli.Namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: debugContainerName,
			Command:   []string{"/bin/sh", "-c", dataCmd},
			Stdin:     false,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, scheme.ParameterCodec)

	ioStreams := IOStreams{
		In:     nil,
		Out:    shardFileWriter, // Stream directly to zip writer
		ErrOut: os.Stderr,
	}

	err = execRemoteKubernetesCommand(ctx, kubeCli.RestConfig, req.URL(), ioStreams, false, false)
	if err != nil {
		return "", fmt.Errorf("shard %d data export failed: %v", shard.ShardIndex, err)
	}
	log.Debug().Str("file", shardFileName).Msg("Shard data streamed directly to zip archive")

	return shardFileName, nil
}

// validateShardFile validates an uncompressed shard file by checking its content
func (o *databaseExportOpts) validateShardFile(file *zip.File) error {
	// Open the file for reading
	reader, err := file.Open()
	if err != nil {
		return fmt.Errorf("failed to open file for reading: %v", err)
	}
	defer reader.Close()

	// Read a portion to validate it contains SQL dump data
	buffer := make([]byte, 8192) // 8KB buffer for validation
	n, err := reader.Read(buffer)
	if err != nil && err != io.EOF {
		return fmt.Errorf("failed to read shard file: %v", err)
	}

	if n == 0 {
		return fmt.Errorf("shard file appears to be empty")
	}

	content := string(buffer[:n])

	// Basic validation - check for SQL dump patterns
	sqlPatterns := []string{"INSERT INTO", "CREATE TABLE", "LOCK TABLES", "UNLOCK TABLES", "mysqldump", "-- Dump completed"}
	hasSQL := false
	for _, pattern := range sqlPatterns {
		if strings.Contains(strings.ToUpper(content), strings.ToUpper(pattern)) {
			hasSQL = true
			break
		}
	}

	if !hasSQL {
		return fmt.Errorf("shard file does not appear to contain valid SQL dump data")
	}

	log.Debug().Int("content_sample_size", n).Msg("Successfully validated shard file content")
	return nil
}

// validateGzippedShardFile validates a gzipped shard file by attempting to decompress it
func (o *databaseExportOpts) validateGzippedShardFile(file *zip.File) error {
	// Open the file for reading
	reader, err := file.Open()
	if err != nil {
		return fmt.Errorf("failed to open file for reading: %v", err)
	}
	defer reader.Close()

	// Create a gzip reader to decompress the content (equivalent to 'gzip -d')
	gzipReader, err := gzip.NewReader(reader)
	if err != nil {
		return fmt.Errorf("failed to create gzip reader: %v", err)
	}
	defer gzipReader.Close()

	// Read through the entire file to validate it can be decompressed
	// This is equivalent to running 'gzip -d' and checking for errors
	buffer := make([]byte, 65536) // 64KB buffer for efficient reading
	totalBytes := int64(0)

	for {
		n, err := gzipReader.Read(buffer)
		if err != nil {
			if err == io.EOF {
				break // End of file reached successfully
			}
			return fmt.Errorf("failed to decompress gzipped file: %v", err)
		}
		totalBytes += int64(n)
	}

	if totalBytes == 0 {
		return fmt.Errorf("gzipped file appears to be empty after decompression")
	}

	log.Debug().Int64("decompressed_bytes", totalBytes).Msg("Successfully validated gzipped file integrity")
	return nil
}

// validateZipFileEntry validates a single file entry in the zip archive
func (o *databaseExportOpts) validateZipFileEntry(file *zip.File) error {
	// Open the file for reading
	reader, err := file.Open()
	if err != nil {
		return fmt.Errorf("failed to open file for reading: %v", err)
	}
	defer reader.Close()

	// Validate based on file type
	switch {
	case file.Name == "export_metadata.json":
		return o.validateMetadataFile(reader)
	case file.Name == "schema.sql":
		return o.validateSchemaFile(reader)
	case regexp.MustCompile(`^shard_\d+\.sql\.gz$`).MatchString(file.Name):
		return o.validateGzippedSQLFile(reader, file.Name)
	default:
		log.Warn().Str("file_name", file.Name).Msg("Unknown file type, skipping validation")
		return nil
	}
}

// validateMetadataFile validates the JSON metadata file
func (o *databaseExportOpts) validateMetadataFile(reader io.ReadCloser) error {
	// Try to decode the JSON to ensure it's valid
	var metadata map[string]interface{}
	decoder := json.NewDecoder(reader)
	if err := decoder.Decode(&metadata); err != nil {
		return fmt.Errorf("invalid JSON metadata: %v", err)
	}

	// Check for required fields
	requiredFields := []string{"environment", "timestamp", "export_options"}
	for _, field := range requiredFields {
		if _, exists := metadata[field]; !exists {
			return fmt.Errorf("missing required field '%s' in metadata", field)
		}
	}

	return nil
}

// validateSchemaFile validates the SQL schema file
func (o *databaseExportOpts) validateSchemaFile(reader io.ReadCloser) error {
	// Read a small portion to check if it looks like SQL
	buffer := make([]byte, 1024)
	n, err := reader.Read(buffer)
	if err != nil && err != io.EOF {
		return fmt.Errorf("failed to read schema file: %v", err)
	}

	content := string(buffer[:n])

	// Basic validation - check for SQL keywords
	sqlKeywords := []string{"CREATE", "TABLE", "INSERT", "DROP", "ALTER"}
	hasSQL := false
	for _, keyword := range sqlKeywords {
		if strings.Contains(strings.ToUpper(content), keyword) {
			hasSQL = true
			break
		}
	}

	if !hasSQL {
		return fmt.Errorf("schema file does not appear to contain valid SQL")
	}

	return nil
}

// validateGzippedSQLFile validates a gzipped SQL file
func (o *databaseExportOpts) validateGzippedSQLFile(reader io.ReadCloser, fileName string) error {
	// Create a gzip reader to decompress the content
	gzipReader, err := gzip.NewReader(reader)
	if err != nil {
		return fmt.Errorf("failed to create gzip reader: %v", err)
	}
	defer gzipReader.Close()

	// Read a small portion to validate it's decompressible and contains SQL
	buffer := make([]byte, 1024)
	n, err := gzipReader.Read(buffer)
	if err != nil && err != io.EOF {
		return fmt.Errorf("failed to decompress gzipped file: %v", err)
	}

	if n == 0 {
		return fmt.Errorf("gzipped file appears to be empty")
	}

	content := string(buffer[:n])

	// Basic validation - check for SQL dump patterns
	sqlPatterns := []string{"INSERT INTO", "CREATE TABLE", "LOCK TABLES", "UNLOCK TABLES", "mysqldump"}
	hasSQL := false
	for _, pattern := range sqlPatterns {
		if strings.Contains(strings.ToUpper(content), strings.ToUpper(pattern)) {
			hasSQL = true
			break
		}
	}

	if !hasSQL {
		return fmt.Errorf("gzipped file does not appear to contain valid SQL dump data")
	}

	return nil
}
