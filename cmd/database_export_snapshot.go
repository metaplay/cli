/*
 * Copyright Metaplay. Licensed under the Apache-2.0 license.
 */

package cmd

import (
	"archive/zip"
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"regexp"
	"strings"
	"time"

	"github.com/metaplay/cli/pkg/envapi"
	"github.com/metaplay/cli/pkg/helmutil"
	"github.com/metaplay/cli/pkg/kubeutil"
	"github.com/metaplay/cli/pkg/styles"
	"github.com/rs/zerolog/log"
	"github.com/spf13/cobra"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/kubernetes/scheme"
)

// databaseExportSnapshotOpts holds the options for the 'database export' command
type databaseExportSnapshotOpts struct {
	UsePositionalArgs

	// Environment and output file
	argEnvironment string
	argOutputFile  string

	// Flags
	flagForce bool
}

func init() {
	o := databaseExportSnapshotOpts{}

	args := o.Arguments()
	args.AddStringArgument(&o.argEnvironment, "ENVIRONMENT", "Target environment name or id, eg, 'lovely-wombats-build-nimbly'.")
	args.AddStringArgumentOpt(&o.argOutputFile, "OUTPUT_FILE", "Output file path for the database snapshot. Defaults to 'database-snapshot-${env}-${timestamp}.mdb' if not specified.")

	cmd := &cobra.Command{
		Use:   "export-snapshot [ENVIRONMENT] [OUTPUT_FILE] [flags]",
		Short: "[preview] Export database snapshot from an environment",
		Long: renderLong(&o, `
			PREVIEW: This is a preview feature and interface may change in the future.

			Export a database snapshot from the specified environment into a file.

			DISCLAIMER: This operation is not a real backup and is intended for ad hoc database
			exports only. This operation should only be used for relatively small databases as it
			copies the entire database to the machine running the CLI.

			The generated snapshot is a zip file containing metadata about the snapshot and a compressed
			SQL dump for each shard, generated by mariadb-dump. The format of the snapshot is subject
			to change in the future.

			This command starts a temporary debug pod and runs mariadb-dump inside it, connects
			to the read-only replica of each shard of the database and creates a complete snapshot.

			WARNING: It is DANGEROUS to export a database while a game server is deployed. When
			exporting a database that is under load, it is possible for the database server's
			purge queue to start growing unboundedly. Do not use the --force on a database with
			non-trivial load on it!

			{Arguments}

			Related commands:
			- 'metaplay database import-snapshot' imports a database snapshot into an environment.
			- 'metaplay debug database' connects to a database shard interactively.
		`),
		Example: renderExample(`
			# Export database from 'nimbly' environment (uses default filename)
			metaplay database export-snapshot nimbly

			# Export database to a specific file
			metaplay database export-snapshot nimbly my_database_snapshot.mdb
		`),
		Run: runCommand(&o),
	}

	cmd.Flags().BoolVar(&o.flagForce, "force", false, "Proceed with export even if a game server is deployed (DANGEROUS!)")

	databaseCmd.AddCommand(cmd)
}

func (o *databaseExportSnapshotOpts) Prepare(cmd *cobra.Command, args []string) error {
	// Generate default output file if not specified
	if o.argOutputFile == "" {
		timestamp := time.Now().Format("20060102-150405")
		o.argOutputFile = fmt.Sprintf("database-snapshot-%s-%s.mdb", o.argEnvironment, timestamp)
	}
	return nil
}

func (o *databaseExportSnapshotOpts) Run(cmd *cobra.Command) error {
	// Resolve the project & auth provider
	project, err := tryResolveProject()
	if err != nil {
		return err
	}

	// Resolve environment config
	envConfig, tokenSet, err := resolveEnvironment(cmd.Context(), project, o.argEnvironment)
	if err != nil {
		return err
	}

	// Resolve target environment & game server
	targetEnv := envapi.NewTargetEnvironment(tokenSet, envConfig.StackDomain, envConfig.HumanID)

	// Create Kubernetes client.
	kubeCli, err := targetEnv.GetPrimaryKubeClient()
	if err != nil {
		return err
	}

	// Configure Helm to check for active deployments
	actionConfig, err := helmutil.NewActionConfig(kubeCli.KubeConfig, envConfig.GetKubernetesNamespace())
	if err != nil {
		return fmt.Errorf("failed to initialize Helm config: %v", err)
	}

	// Check for any active game server Helm deployments - refuse to export if found
	helmReleases, err := helmutil.HelmListReleases(actionConfig, "metaplay-gameserver")
	if err != nil {
		return fmt.Errorf("failed to check for existing Helm releases: %v", err)
	}
	hasGameServer := len(helmReleases) > 0

	// Fetch the database shard configuration from Kubernetes secret
	log.Debug().Str("namespace", kubeCli.Namespace).Msg("Fetching database shard configuration")
	shards, err := kubeutil.FetchDatabaseShardsFromSecret(cmd.Context(), kubeCli, kubeCli.Namespace)
	if err != nil {
		return err
	}

	// Show info
	log.Info().Msg("")
	log.Info().Msg("Export database snapshot:")
	log.Info().Msgf("  Environment: %s", styles.RenderTechnical(o.argEnvironment))
	if hasGameServer {
		log.Info().Msgf("  Game server: %s", styles.RenderWarning("⚠️ deployed"))
	} else {
		log.Info().Msgf("  Game server: %s", styles.RenderSuccess("✓ not deployed"))
	}
	log.Info().Msgf("  Shards:      %s", styles.RenderTechnical(fmt.Sprintf("%d", len(shards))))
	log.Info().Msgf("  Output file: %s", styles.RenderTechnical(o.argOutputFile))
	log.Info().Msg("")

	// Check if there's a game server deployed.
	if hasGameServer {
		if !o.flagForce {
			return fmt.Errorf("cannot export database: active game server deployment detected in environment '%s'. Remove the game server deployment before exporting the database", o.argEnvironment)
		}

		log.Info().Msgf("%s %s", styles.RenderWarning("⚠️"), fmt.Sprintf("WARNING: active game server deployment detected in environment '%s'", o.argEnvironment))
		log.Info().Msgf("   Proceeding with database export due to --force flag")
		log.Info().Msg("")
	}

	// Create a debug container to run mariadb-dump
	log.Debug().Msg("Creating debug pod for database export")
	podName, cleanup, err := kubeutil.CreateDebugPod(
		cmd.Context(),
		kubeCli,
		debugDatabaseImage,
		false,
		false,
		[]string{"sleep", "3600"},
	)
	if err != nil {
		return err
	}
	log.Debug().Str("pod_name", podName).Msg("Debug pod created successfully")
	// Make sure the debug container is cleaned up even if we return early
	defer cleanup()

	// Export the database
	log.Debug().Str("output_file", o.argOutputFile).Msg("Start database export")
	err = o.exportDatabaseContents(cmd.Context(), kubeCli, podName, "debug", shards)
	if err != nil {
		// Hard failure - remove incomplete file
		os.Remove(o.argOutputFile)
		log.Error().Err(err).Msg("Database export failed - removing incomplete zip file")
		return fmt.Errorf("CRITICAL: database export failed, zip file removed: %v", err)
	}

	log.Info().Msg("")
	log.Info().Msgf("✅ Database export completed successfully")

	return nil
}

// DatabaseSnapshotMetadata contains information about the database export
type DatabaseSnapshotMetadata struct {
	Version      int       `json:"version"`
	Environment  string    `json:"environment"`
	DatabaseName string    `json:"database_name"`
	NumShards    int       `json:"num_shards"`
	ExportedAt   time.Time `json:"exported_at"`
}

// Main function to export database contents - creates zip file, writes metadata, and exports all shards
func (o *databaseExportSnapshotOpts) exportDatabaseContents(ctx context.Context, kubeCli *envapi.KubeClient, podName, debugContainerName string, shards []kubeutil.DatabaseShardConfig) error {
	log.Info().Msgf("Exporting database...")

	// Create output zip file
	log.Debug().Str("zip_file", o.argOutputFile).Msg("Creating output zip file")
	zipFile, err := os.Create(o.argOutputFile)
	if err != nil {
		return fmt.Errorf("failed to create zip file: %v", err)
	}
	defer zipFile.Close()

	// Create zip writer
	zipWriter := zip.NewWriter(zipFile)
	defer zipWriter.Close()

	// Write metadata to zip
	log.Debug().Msg("Writing metadata to zip file")
	err = o.writeMetadataToZip(zipWriter, shards)
	if err != nil {
		return fmt.Errorf("failed to write metadata: %v", err)
	}

	// Extract schema from shard #0 first
	log.Debug().Msg("Extracting database schema from shard #0")
	schemaContent, err := o.extractDatabaseSchema(ctx, kubeCli, podName, debugContainerName, shards[0])
	if err != nil {
		return fmt.Errorf("failed to extract schema: %v", err)
	}

	// Apply schema fixups
	schemaContent = o.applySchemaFixups(schemaContent)

	// Write schema to zip file
	err = o.writeSchemaToZip(zipWriter, schemaContent)
	if err != nil {
		return fmt.Errorf("failed to write schema to zip: %v", err)
	}

	// Export data from each shard
	var shardFileNames []string
	for _, shard := range shards {
		log.Debug().Int("shard_index", shard.ShardIndex).Str("database_name", shard.DatabaseName).Msg("Starting shard data export")
		shardFileName, err := o.exportDatabaseShardData(ctx, zipWriter, kubeCli, podName, debugContainerName, shard)
		if err != nil {
			return fmt.Errorf("failed to export shard %d data: %v", shard.ShardIndex, err)
		}
		log.Debug().Int("shard_index", shard.ShardIndex).Str("shard_file", shardFileName).Msg("Shard data export completed")
		shardFileNames = append(shardFileNames, shardFileName)
	}

	return nil
}

// Helper function to write metadata to zip file
func (o *databaseExportSnapshotOpts) writeMetadataToZip(zipWriter *zip.Writer, shards []kubeutil.DatabaseShardConfig) error {

	// Use first shard for database name (all shards should have same database name)
	databaseName := ""
	if len(shards) > 0 {
		databaseName = shards[0].DatabaseName
	}

	// Create metadata
	log.Debug().Str("database_name", databaseName).Int("num_shards", len(shards)).Msg("Creating export metadata")
	metadata := DatabaseSnapshotMetadata{
		Version:      1,
		Environment:  o.argEnvironment,
		DatabaseName: databaseName,
		NumShards:    len(shards),
		ExportedAt:   time.Now().UTC(),
	}

	// Create metadata JSON in memory
	metadataBytes, err := json.MarshalIndent(metadata, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal metadata: %v", err)
	}

	// Write metadata file to zip
	metadataFileName := "metadata.json"
	metadataHeader := &zip.FileHeader{
		Name:     metadataFileName,
		Method:   zip.Store, // No compression for small metadata file
		Modified: time.Now(),
	}
	metadataHeader.SetMode(0644)

	// Write metadata file to zip
	metadataWriter, err := zipWriter.CreateHeader(metadataHeader)
	if err != nil {
		return fmt.Errorf("failed to create metadata writer: %v", err)
	}
	if _, err := metadataWriter.Write(metadataBytes); err != nil {
		return fmt.Errorf("failed to write metadata content: %v", err)
	}

	return nil
}

// Helper function to extract database schema from shard #0 into memory
func (o *databaseExportSnapshotOpts) extractDatabaseSchema(ctx context.Context, kubeCli *envapi.KubeClient, podName, debugContainerName string, shard kubeutil.DatabaseShardConfig) (string, error) {
	// Build mariadb-dump command for schema only (DDL) - no gzip compression for in-memory processing
	schemaCmd := fmt.Sprintf("mariadb-dump -h %s -u %s -p%s --no-create-db --no-tablespaces --no-data --routines --triggers %s",
		shard.ReadOnlyHost, // Use read-only replica
		shard.UserId,
		shard.Password,
		shard.DatabaseName)
	log.Debug().Str("host", shard.ReadOnlyHost).Str("database", shard.DatabaseName).Msg("Executing schema dump command")

	// Capture output in memory buffer
	var schemaBuffer bytes.Buffer

	// Execute schema dump command
	req := kubeCli.Clientset.CoreV1().
		RESTClient().
		Post().
		Resource("pods").
		Name(podName).
		Namespace(kubeCli.Namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: debugContainerName,
			Command:   []string{"/bin/sh", "-c", schemaCmd},
			Stdin:     false,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, scheme.ParameterCodec)

	ioStreams := IOStreams{
		In:     nil,
		Out:    &schemaBuffer, // Capture to memory buffer
		ErrOut: os.Stderr,
	}

	err := execRemoteKubernetesCommand(ctx, kubeCli.RestConfig, req.URL(), ioStreams, false, false)
	if err != nil {
		log.Error().Err(err).Msg("Schema extraction failed - aborting export")
		return "", fmt.Errorf("CRITICAL: schema extraction failed: %v", err)
	}

	schemaContent := schemaBuffer.String()
	log.Debug().Int("schema_size", len(schemaContent)).Msg("Schema extracted to memory successfully")

	return schemaContent, nil
}

// Helper function to apply schema modifications and fixups
func (o *databaseExportSnapshotOpts) applySchemaFixups(schemaContent string) string {
	log.Debug().Msgf("Original schema from mariadb-dump:\n%s", schemaContent)

	// Ensure all tables use utf8mb4_bin collation; Metaplay SDK versions before R34 left
	// the __EFMigrationsHisotry table in the database's default collation, which may not
	// be compatible across different database types and versions (e.g., MariaDB vs MySQL).
	schemaContent = o.enforceUtf8mb4BinCollation(schemaContent)

	log.Debug().Msg("Applied schema fixups")
	return schemaContent
}

// Helper function to enforce utf8mb4_bin collation on all tables
func (o *databaseExportSnapshotOpts) enforceUtf8mb4BinCollation(schemaContent string) string {
	// Simple approach: replace all collation references with utf8mb4_bin

	// 1. Replace table-level collation in CREATE TABLE statements
	tableCollationPattern := regexp.MustCompile(`(?i)DEFAULT\s+CHARSET=\w+\s+COLLATE=\w+`)
	result := tableCollationPattern.ReplaceAllString(schemaContent, "DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin")

	// 2. Replace column-level collation specifications
	columnCollationPattern := regexp.MustCompile(`(?i)CHARACTER\s+SET\s+\w+\s+COLLATE\s+\w+`)
	result = columnCollationPattern.ReplaceAllString(result, "CHARACTER SET utf8mb4 COLLATE utf8mb4_bin")

	// 3. Replace standalone COLLATE clauses
	standaloneCollatePattern := regexp.MustCompile(`(?i)COLLATE\s+\w+`)
	result = standaloneCollatePattern.ReplaceAllString(result, "COLLATE utf8mb4_bin")

	// 4. Replace standalone CHARSET clauses
	standaloneCharsetPattern := regexp.MustCompile(`(?i)CHARSET=\w+`)
	result = standaloneCharsetPattern.ReplaceAllString(result, "CHARSET=utf8mb4")

	log.Debug().Msg("Enforced utf8mb4_bin collation on all tables and columns")
	return result
}

// Helper function to write schema content to zip file
func (o *databaseExportSnapshotOpts) writeSchemaToZip(zipWriter *zip.Writer, schemaContent string) error {
	// Prepare zip header for schema file
	schemaHeader := &zip.FileHeader{
		Name:     "schema.sql",
		Method:   zip.Deflate, // Use compression since we're not pre-compressing
		Modified: time.Now(),
	}
	schemaHeader.SetMode(0644)

	// Create zip writer for schema file
	schemaWriter, err := zipWriter.CreateHeader(schemaHeader)
	if err != nil {
		return fmt.Errorf("failed to create schema writer: %v", err)
	}

	// Write schema content to zip
	_, err = schemaWriter.Write([]byte(schemaContent))
	if err != nil {
		return fmt.Errorf("failed to write schema content: %v", err)
	}

	log.Debug().Msg("Schema written to schema.sql in zip archive")
	return nil
}

// Helper function to export data only from a single database shard
func (o *databaseExportSnapshotOpts) exportDatabaseShardData(ctx context.Context, zipWriter *zip.Writer, kubeCli *envapi.KubeClient, podName, debugContainerName string, shard kubeutil.DatabaseShardConfig) (string, error) {
	// Build mariadb-dump command for data only (DML) with proper gzip termination
	// Note: Using --hex-blob as raw binary causes failures on import.
	dataCmd := fmt.Sprintf("mariadb-dump -h %s -u %s -p%s --no-create-info --no-tablespaces --single-transaction --hex-blob --skip-triggers %s",
		shard.ReadOnlyHost, // Use read-only replica
		shard.UserId,
		shard.Password,
		shard.DatabaseName)
	log.Debug().Str("host", shard.ReadOnlyHost).Str("database", shard.DatabaseName).Msg("Executing data dump command")

	// Prepare zip header for streaming
	shardFileName := fmt.Sprintf("shard_%d.sql", shard.ShardIndex)
	shardHeader := &zip.FileHeader{
		Name:     shardFileName,
		Method:   zip.Deflate, // Use compression since data is not pre-compressed
		Modified: time.Now(),
	}
	shardHeader.SetMode(0644)

	// Create zip writer for this file - stream directly without buffering
	shardFileWriter, err := zipWriter.CreateHeader(shardHeader)
	if err != nil {
		return "", fmt.Errorf("failed to create zip writer: %v", err)
	}

	// Execute mariadb-dump command and stream output directly to zip
	req := kubeCli.Clientset.CoreV1().
		RESTClient().
		Post().
		Resource("pods").
		Name(podName).
		Namespace(kubeCli.Namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: debugContainerName,
			Command:   []string{"/bin/sh", "-c", dataCmd},
			Stdin:     false,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, scheme.ParameterCodec)

	ioStreams := IOStreams{
		In:     nil,
		Out:    shardFileWriter, // Stream directly to zip writer
		ErrOut: os.Stderr,
	}

	err = execRemoteKubernetesCommand(ctx, kubeCli.RestConfig, req.URL(), ioStreams, false, false)
	if err != nil {
		return "", fmt.Errorf("shard %d data export failed: %v", shard.ShardIndex, err)
	}
	log.Debug().Str("file", shardFileName).Msg("Shard data streamed directly to zip archive")

	return shardFileName, nil
}

// validateShardFile validates an uncompressed shard file by checking its content
func (o *databaseExportSnapshotOpts) validateShardFile(file *zip.File) error {
	// Open the file for reading
	reader, err := file.Open()
	if err != nil {
		return fmt.Errorf("failed to open file for reading: %v", err)
	}
	defer reader.Close()

	// Read a portion to validate it contains SQL dump data
	buffer := make([]byte, 8192) // 8KB buffer for validation
	n, err := reader.Read(buffer)
	if err != nil && err != io.EOF {
		return fmt.Errorf("failed to read shard file: %v", err)
	}

	if n == 0 {
		return fmt.Errorf("shard file appears to be empty")
	}

	content := string(buffer[:n])

	// Basic validation - check for SQL dump patterns
	sqlPatterns := []string{"INSERT INTO", "CREATE TABLE", "LOCK TABLES", "UNLOCK TABLES", "mysqldump", "-- Dump completed"}
	hasSQL := false
	for _, pattern := range sqlPatterns {
		if strings.Contains(strings.ToUpper(content), strings.ToUpper(pattern)) {
			hasSQL = true
			break
		}
	}

	if !hasSQL {
		return fmt.Errorf("shard file does not appear to contain valid SQL dump data")
	}

	log.Debug().Int("file_size", n).Msg("Successfully validated shard file content")
	return nil
}

// validateZipFileEntry validates a single file entry in the zip archive
func (o *databaseExportSnapshotOpts) validateZipFileEntry(file *zip.File) error {
	// Open the file for reading
	reader, err := file.Open()
	if err != nil {
		return fmt.Errorf("failed to open file for reading: %v", err)
	}
	defer reader.Close()

	// Validate based on file type
	switch {
	case file.Name == "export_metadata.json":
		return o.validateMetadataFile(reader)
	case file.Name == "schema.sql":
		return o.validateSchemaFile(reader)
	case regexp.MustCompile(`^shard_\d+\.sql$`).MatchString(file.Name):
		return o.validateShardFile(file)
	default:
		log.Warn().Str("file_name", file.Name).Msg("Unknown file type, skipping validation")
		return nil
	}
}

// validateMetadataFile validates the JSON metadata file
func (o *databaseExportSnapshotOpts) validateMetadataFile(reader io.ReadCloser) error {
	// Try to decode the JSON to ensure it's valid
	var metadata map[string]interface{}
	decoder := json.NewDecoder(reader)
	if err := decoder.Decode(&metadata); err != nil {
		return fmt.Errorf("invalid JSON metadata: %v", err)
	}

	// Check for required fields
	requiredFields := []string{"environment", "timestamp", "export_options"}
	for _, field := range requiredFields {
		if _, exists := metadata[field]; !exists {
			return fmt.Errorf("missing required field '%s' in metadata", field)
		}
	}

	return nil
}

// validateSchemaFile validates the SQL schema file
func (o *databaseExportSnapshotOpts) validateSchemaFile(reader io.ReadCloser) error {
	// Read a small portion to check if it looks like SQL
	buffer := make([]byte, 1024)
	n, err := reader.Read(buffer)
	if err != nil && err != io.EOF {
		return fmt.Errorf("failed to read schema file: %v", err)
	}

	content := string(buffer[:n])

	// Basic validation - check for SQL keywords
	sqlKeywords := []string{"CREATE", "TABLE", "INSERT", "DROP", "ALTER"}
	hasSQL := false
	for _, keyword := range sqlKeywords {
		if strings.Contains(strings.ToUpper(content), keyword) {
			hasSQL = true
			break
		}
	}

	if !hasSQL {
		return fmt.Errorf("schema file does not appear to contain valid SQL")
	}

	return nil
}
